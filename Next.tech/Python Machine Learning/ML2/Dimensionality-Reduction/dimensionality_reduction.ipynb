{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from helper import plot_decision_regions\n",
    "from rbf_kernel_pca import rbf_kernel_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wine dataset as pandas dataframe\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "                      'machine-learning-databases/wine/wine.data',\n",
    "                      header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing sets\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3,\n",
    "    stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "# standardize the features\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# obtain eigenpairs of covariance matrix\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "print(f'\\nEigenvalues \\n{eigen_vals}')\n",
    "print(f'\\nEigenvectors \\n{eigen_vecs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cumulative sum of explained variances\n",
    "tot = sum(eigen_vals)\n",
    "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# plot explained variances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(1,14), var_exp, alpha=0.5,\n",
    "        align='center', label='individual explained variance')\n",
    "plt.step(range(1,14), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect two eigenvoctors that correspond to two largest eigenvalues\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform sample x onto PCA subspace\n",
    "X_train_std[0].dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform entire training dataset\n",
    "X_train_pca = X_train_std.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformed dataset\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "plt.figure(figsize=(10, 10))\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train==l, 0], \n",
    "                X_train_pca[y_train==l, 1], \n",
    "                c=c, label=l, marker=m) \n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# intialize pca and logistic regression model\n",
    "pca = PCA(n_components=2)\n",
    "lr = LogisticRegression(multi_class='auto', solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform data\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "lr.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision regions for training set\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision regions for test set\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mean vectors\n",
    "np.set_printoptions(precision=4)\n",
    "mean_vecs = []\n",
    "for label in range(1,4):\n",
    "    mean_vecs.append(np.mean(\n",
    "               X_train_std[y_train==label], axis=0))\n",
    "    print(f'MV {label}: {mean_vecs[label-1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute within-class scatter matrix\n",
    "d = 13  # number of features\n",
    "S_W = np.zeros((d, d))\n",
    "for label, mv in zip(range(1, 4), mean_vecs):\n",
    "    class_scatter = np.zeros((d, d))\n",
    "    for row in X_train_std[y_train == label]:\n",
    "        row, mv = row.reshape(d, 1), mv.reshape(d, 1)  \n",
    "        class_scatter += (row - mv).dot((row - mv).T)\n",
    "    S_W += class_scatter\n",
    "print(f'Within-class scatter matrix: {S_W.shape[0]}x{S_W.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Class label distribution: {np.bincount(y_train)[1:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale individual scatter matrices before computing within-class matrix\n",
    "d = 13  # number of features\n",
    "S_W = np.zeros((d, d))\n",
    "for label,mv in zip(range(1, 4), mean_vecs):\n",
    "    class_scatter = np.cov(X_train_std[y_train==label].T)\n",
    "    S_W += class_scatter\n",
    "print(f'Scaled within-class scatter matrix: {S_W.shape[0]}x{S_W.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute between-class scatter matrix\n",
    "mean_overall = np.mean(X_train_std, axis=0)\n",
    "d = 13  # number of features\n",
    "S_B = np.zeros((d, d))\n",
    "for i, mean_vec in enumerate(mean_vecs):\n",
    "    n = X_train[y_train == i + 1, :].shape[0]\n",
    "    mean_vec = mean_vec.reshape(d, 1)  # make column vector\n",
    "    mean_overall = mean_overall.reshape(d, 1) \n",
    "    S_B += n * (mean_vec - mean_overall).dot(\n",
    "               (mean_vec - mean_overall).T)\n",
    "print(f'Between-class scatter matrix: {S_B.shape[0]}x{S_B.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eigenpairs\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort eigenvalues\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) \n",
    "             for i in range(len(eigen_vals))]\n",
    "eigen_pairs = sorted(eigen_pairs, \n",
    "              key=lambda k: k[0], reverse=True)\n",
    "print('Eigenvalues in descending order:\\n')\n",
    "for eigen_val in eigen_pairs:\n",
    "    print(eigen_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot eigenvectors by decreasing eigenvalues\n",
    "tot = sum(eigen_vals.real)\n",
    "discr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]\n",
    "cum_discr = np.cumsum(discr)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(1, 14), discr, alpha=0.5, align='center',\n",
    "        label='individual \"discriminability\"')\n",
    "plt.step(range(1, 14), cum_discr, where='mid',\n",
    "         label='cumulative \"discriminability\"')\n",
    "plt.ylabel('\"discriminability\" ratio')\n",
    "plt.xlabel('Linear Discriminants')\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transformation matrix\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,\n",
    "               eigen_pairs[1][1][:, np.newaxis].real))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot subspace\n",
    "X_train_lda = X_train_std.dot(w)\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "plt.figure(figsize=(10, 10))\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_lda[y_train==l, 0], \n",
    "                X_train_lda[y_train==l, 1] * (-1), \n",
    "                c=c, label=l, marker=m)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit logistic regression model\n",
    "lr = LogisticRegression(multi_class='auto', solver='liblinear')\n",
    "lr = lr.fit(X_train_lda, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision regions for training set\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_decision_regions(X_train_lda, y_train, classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision regions for test set\n",
    "X_test_lda = lda.transform(X_test_std)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_decision_regions(X_test_lda, y_test, classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating half-moon shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PCA\n",
    "scikit_pca = PCA(n_components=2)\n",
    "X_spca = scikit_pca.fit_transform(X)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "ax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "ax[1].scatter(X_spca[y == 0, 0], np.zeros((50, 1))+0.02, color='red', marker='^', alpha=0.5)\n",
    "ax[1].scatter(X_spca[y == 1, 0], np.zeros((50, 1))-0.02, color='blue', marker='o', alpha=0.5)\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].set_ylim([-1, 1])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel PCA\n",
    "X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "ax[0].scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "ax[1].scatter(X_kpca[y == 0, 0], np.zeros((50, 1))+0.02, color='red', marker='^', alpha=0.5)\n",
    "ax[1].scatter(X_kpca[y == 1, 0], np.zeros((50, 1))-0.02, color='blue', marker='o', alpha=0.5)\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].set_ylim([-1, 1])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating concentric circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PCA\n",
    "scikit_pca = PCA(n_components=2)\n",
    "X_spca = scikit_pca.fit_transform(X)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "ax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "ax[1].scatter(X_spca[y == 0, 0], np.zeros((500,1))+0.02, color='red', marker='^', alpha=0.5)\n",
    "ax[1].scatter(X_spca[y == 1, 0], np.zeros((500,1))-0.02, color='blue', marker='o', alpha=0.5)\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].set_ylim([-1, 1])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel PCA\n",
    "X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(15, 5))\n",
    "ax[0].scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1],color='blue', marker='o', alpha=0.5)\n",
    "ax[1].scatter(X_kpca[y == 0, 0], np.zeros((500, 1))+0.02, color='red', marker='^', alpha=0.5)\n",
    "ax[1].scatter(X_kpca[y == 1, 0], np.zeros((500, 1))-0.02, color='blue', marker='o', alpha=0.5)\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].set_ylim([-1, 1])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting New Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rbf_kernel_pca2 import rbf_kernel_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "alphas, lambdas = rbf_kernel_pca(X, gamma=15, n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26th point in dataset as x'\n",
    "x_new = X[25]\n",
    "\n",
    "x_proj = alphas[25]  # original projection\n",
    "\n",
    "\n",
    "def project_x(x_new, X, gamma, alphas, lambdas):\n",
    "    \"\"\"Project new x onto new subspace.\"\"\"\n",
    "    pair_dist = np.array([np.sum((x_new-row)**2) for row in X])\n",
    "    k = np.exp(-gamma * pair_dist)\n",
    "    return k.dot(alphas / lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project\n",
    "x_reproj = project_x(x_new, X, gamma=15, alphas=alphas, lambdas=lambdas)\n",
    "x_reproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(alphas[y == 0, 0], np.zeros((50)), color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(alphas[y == 1, 0], np.zeros((50)), color='blue', marker='o', alpha=0.5)\n",
    "plt.scatter(x_proj, 0, color='black', label='original projection of point X[25]', marker='^', s=100)\n",
    "plt.scatter(x_reproj, 0, color='green', label='remapped point X[25]', marker='x', s=500)\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA in `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "scikit_kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\n",
    "X_skernpca = scikit_kpca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_skernpca[y == 0, 0], X_skernpca[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X_skernpca[y == 1, 0], X_skernpca[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
